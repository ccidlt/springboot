server:
  port: 8080
#  servlet:
#    context-path: /springboot
spring:
  profiles:
    active: dev
  application:
    name: springboot
  datasource:
    driver-class-name: com.mysql.cj.jdbc.Driver
    url: jdbc:mysql://127.0.0.1:3306/test?useUnicode=true&characterEncoding=utf8&useSSL=false
    username: root
    password: 123456
    type: com.alibaba.druid.pool.DruidDataSource
    # 连接池配置
    druid:
      # 初始化大小，最小，最大
      initial-size: 5
      min-idle: 5
      max-active: 20
      # 配置获取连接等待超时的时间
      max-wait: 60000
      # 配置间隔多久才进行一次检测，检测需要关闭的空闲连接，单位毫秒
      time-between-eviction-runs-millis: 60000
      # 配置一个连接在池中最小生存时间
      min-evictable-idle-time-millis: 300000
      validation-query: select 'x'
      test-while-idle: true
      test-on-borrow: false
      test-on-return: false
      # 打开 PSCache，并且指定每个连接上 PSCache 的大小
      pool-prepared-statements: true
      max-pool-prepared-statement-per-connection-size: 20
      # 配置监控统计拦截的 Filter，去掉后监控界面 SQL 无法统计，wall 用于防火墙
      filters: stat,wall,slf4j
      # 通过 connection-properties 属性打开 mergeSql 功能；慢 SQL 记录
      connection-properties: druid.stat.mergeSql\=true;druid.stat.slowSqlMillis\=5000
  cache:
    type: redis
  redis:
    #Redis服务器连接地址和端口
    host: 127.0.0.1
    port: 6379
    #Redis服务器连接密码（默认为空）
    password:
    jedis:
      pool:
        #连接池最大连接数（使用负值表示没有限制）
        max-active: 8
        #连接池最大阻塞等待时间（使用负值表示没有限制）
        max-wait: -1
        #连接池中的最大空闲连接
        max-idle: 8
        #连接池中的最小空闲连接
        min-idle: 0
#    lettuce:
#      pool:
#        max-active: 8
#        max-wait: -1
#        max-idle: 8
#        min-idle: 0
    #连接超时时间（毫秒）
    timeout: 30000
  servlet:
    multipart:
      max-file-size: 1024MB

#  kafka:
#    bootstrap-servers: 127.0.0.1:9092
#    #producer生产者配置序列化
#    producer:
#      key-serializer: org.apache.kafka.common.serialization.StringSerializer
#      value-serializer: org.apache.kafka.common.serialization.StringSerializer
#      batch-size: 16384 #生产者每个批次最多放多少条记录
#      buffer-memory: 33554432 #生产者一端总的可用发送缓存区大小，此处设置为32M
#    #consumer消费者配置序列化
#    consumer:
#      key-serializer: org.apache.kafka.common.serialization.StringDeserializer
#      value-serializer: org.apache.kafka.common.serialization.StringDeserializer
#      group-id: springboot-consumer123
#      auto-offset-reset: earliest #如果在kafka中找不到当前消费者的偏移量，则直接将偏移量重置为最早的
#      enable-auto-commit: true #消费者的偏移量是自动提交还是手动提交，此处自动提交偏移量
#      auto-commit-interval: 1000 #消费者偏移量自动提交的时间间隔

#  rabbitmq:
#    host: 127.0.0.1
#    port: 5672
#    username: guest
#    password: guest


logging:
  level:
    root: INFO
    com.ds: DEBUG
  file:
    path: log
  pattern:
    console: "%d{yyyy/MM/dd-HH:mm:ss} [%thread] %-5level %logger- %msg%n"

mybatis-plus:
  mapper-locations: classpath:mapper/*.xml
  type-aliases-package: com.ds.entity
  configuration:
    map-underscore-to-camel-case: true #t对应属性可驼峰命名
    cache-enabled: true #开启全局二级缓存
    call-setters-on-nulls: true #resultMap返回的是Map时，返回值为空的键值关系
    log-impl: org.apache.ibatis.logging.stdout.StdOutImpl #sql语句详情打印
  global-config:
    db-config:
      logic-delete-value: 1 # 逻辑已删除值(默认为 1)
      logic-not-delete-value: 0 # 逻辑未删除值(默认为 0)

#PageHelper分页
pagehelper:
  helperDialect: mysql
  offsetAsPageNum: true
  rowBoundsWithCount: true
  supportMethodsArguments: true
  reasonable: true
  pageSizeZero: false #pageSize=0
  params: count=countSql
