server:
  port: 8078
  servlet:
    context-path: /
  # 开启Gzip压缩，默认只压缩超过2048字节的数据
  compression:
    enabled: true
    mime-types: application/json
spring:
  application:
    name: springboot
  jackson:
    date-format: yyyy-MM-dd HH:mm:ss
    time-zone: GMT+8
  datasource:
#    driver-class-name: com.mysql.cj.jdbc.Driver
#    url: jdbc:mysql://127.0.0.1:3306/test?useUnicode=true&characterEncoding=utf8&useSSL=false&serverTimezone=GMT%2B8
#    username: root
#    password: 123456
    dynamic:
      primary: master #设置默认的数据源或者数据源组,默认值即为master
      datasource:
        master:
          driver-class-name: com.mysql.cj.jdbc.Driver
          url: jdbc:mysql://127.0.0.1:3306/test?useUnicode=true&characterEncoding=utf8&useSSL=false&serverTimezone=GMT%2B8
          username: root
          password: 123456
        slave:
          driver-class-name: com.mysql.cj.jdbc.Driver
          url: jdbc:mysql://127.0.0.1:3306/test?useUnicode=true&characterEncoding=utf8&useSSL=false&serverTimezone=GMT%2B8
          username: root
          password: 123456
  cache:
    type: redis
  redis:
    #Redis服务器连接地址和端口
    host: 127.0.0.1
    port: 6379
    #Redis服务器连接密码（默认为空）
    password:
    jedis:
      pool:
        #连接池最大连接数（使用负值表示没有限制）
        max-active: 8
        #连接池最大阻塞等待时间（使用负值表示没有限制）
        max-wait: -1
        #连接池中的最大空闲连接
        max-idle: 8
        #连接池中的最小空闲连接
        min-idle: 0
    #    lettuce:
    #      pool:
    #        max-active: 8
    #        max-wait: -1
    #        max-idle: 8
    #        min-idle: 0
    #连接超时时间（毫秒）
    timeout: 30000
  servlet:
    multipart:
      max-file-size: 1024MB

  kafka:
    bootstrap-servers: 127.0.0.1:9092
    #producer生产者配置序列化
    producer:
      key-serializer: org.apache.kafka.common.serialization.StringSerializer
      value-serializer: org.apache.kafka.common.serialization.StringSerializer
      batch-size: 16384 #生产者每个批次最多放多少条记录
      buffer-memory: 33554432 #生产者一端总的可用发送缓存区大小，此处设置为32M
    #consumer消费者配置序列化
    consumer:
      key-deserializer: org.apache.kafka.common.serialization.StringDeserializer
      value-deserializer: org.apache.kafka.common.serialization.StringDeserializer
      group-id: defaultGroupId
      auto-offset-reset: earliest #如果在kafka中找不到当前消费者的偏移量，则直接将偏移量重置为最早的
      #enable-auto-commit: true #消费者的偏移量是自动提交还是手动提交，此处自动提交偏移量
      #auto-commit-interval: 1000 #消费者偏移量自动提交的时间间隔
      enable-auto-commit: false #手动提交偏移量
      max-poll-records: 1000 #最大消费100条
    listener:
      type: batch #开启批量消费
      concurrency: 1 #同时处理线程数
      poll-timeout: 1500 #只限自动提交
      missing-topics-fatal: false #无主题不报异常
      ack-mode: manual #当每一批poll()的数据被消费者监听器（ListenerConsumer）处理之后, 手动调用Acknowledgment.acknowledge()后提交

#  rabbitmq:
#    host: 127.0.0.1
#    port: 5672
#    username: guest
#    password: guest


#logging:
#  level:
#    root: INFO
#    com.ds: DEBUG
#  file:
#    path: log
#  pattern:
#    console: "%d{yyyy/MM/dd-HH:mm:ss} [%thread] %-5level %logger- %msg%n"

mybatis-plus:
  mapper-locations: classpath:mapper/*.xml
  type-aliases-package: com.ds.entity
  configuration:
    map-underscore-to-camel-case: true #t对应属性可驼峰命名
    cache-enabled: true #开启全局二级缓存
    call-setters-on-nulls: true #resultMap返回的是Map时，返回值为空的键值关系
#    log-impl: org.apache.ibatis.logging.stdout.StdOutImpl #sql语句详情打印
  global-config:
    db-config:
      logic-delete-value: 1 # 逻辑已删除值(默认为 1)
      logic-not-delete-value: 0 # 逻辑未删除值(默认为 0)

#PageHelper分页
pagehelper:
  helperDialect: mysql
  offsetAsPageNum: true
  rowBoundsWithCount: true
  supportMethodsArguments: true
  reasonable: true
  pageSizeZero: false #pageSize=0
  params: count=countSql

#文件路径
filePath: /home/file/

#kafka topic
kafka_topics: topic01,topic-new-1

#redis消息队列
redislist:
  mq1: mq1
  mq2: mq2
  mq3: mq3

#雪花算法
snow-flake:
  dataCenterId: 0 #数据中心
  machineId: 0 #机器标识
